\section{Introduction}

Nature-inspired optimization algorithms have been used successfully in the last
decades to tackle complex problems \cite{yang2014nature}. These
algorithms include evolutionary algorithms (EAs)
\cite{back1996evolutionary} and swarm intelligence (SI)
\cite{kennedy2006swarm}. % Maybe mention a few more, like bee o others
                         % like that? - JJ 
                         % ok, we have them by the dozen - Mario
Genetic Algorithms (GAs) 
\cite{holland1992adaptation,eiben2003genetic}, 
Differential Evolution (DE) \cite{karabouga2004simple}, and 
Genetic Programming (GP) \cite{back1996evolutionary} are popular EAs, while examples of SI algorithms 
\cite{kennedy2006swarm} are particle swarm optimization (PSO)
\cite{clerc2010particle}, artificial bee colony (ABC) \cite{karaboga2005idea} and Grey Wolf Optimization
(GWO) \cite{mirjalili2014grey}, and ant colony algorithms (ACO) \cite{dorigo1999ant}.
A common characteristic of these kind of methods is the
use of an initial set of random candidate solutions that are manipulated by the
algorithm to generate a new set of candidates, and because of this, we commonly
referred to them as population-based algorithms. % There are other
                                % population based algorithms that are
                                % not natural, such as PBIL. Maybe you
                                % could mention them - JJ
There are other stochastic optimization methods, that even if they are not
nature-inspired, they employ the concept of an evolving population. An example
of this kind of method is population-based incremental learning (PBIL)
\cite{baluja1994population}, which is an optimization and an estimation of
distribution algorithm. estimation of distribution algorithm.
Since instead of piecewise constructing a population or changing it
incrementally, all members of the population have to be evaluated to
obtain a fitnen of each population.  In this paper, we are interested in the
latter, that is, having multiple populations running with distinct parameters
and optimization algorithms. We can find in the literature, several
heterogeneous e it is difficult to overcome that $n$ factor, where $n$
is the population size, in the complexity of every iteration of the
algorithm. That is why researchers have been proposing some form of
parallelization since early on \cite{muhlenbein1988evolution} to
decrease their execution time. % Not scalability, but wallclock timing - JJ
One of the first methods of parallelization was the island
model, which lead to an increased performance
\cite{gorges1990explicit,grosso1985computer}. The concept was to divide the
population into smaller populations that communicated with each other. Other
population-based algorithms have adopted the technique, and since then,
researchers have found additional advantages besides the execution speed, 
these include avoiding a premature convergence and maintaining the diversity of the
global population \cite{li2015multi}, we are going to call these methods 
multi-population algorithms \cite{Ma2019}. The relative 
isolation in which populations carry out the algorithm, together with the
synchronous or asynchronous communication, helps to increase the overall
diversity since each population will search in a particular area, at least
between communications. Multi-population algorithms use a form of
communication to recombine or migrate candidate solutions between populations
to avoid premature convergence, since smaller populations are known to perform
better for a given problem than bigger populations
\cite{li2016multi,wu2016differential}.  Even in some cases, a multi-population based
algorithm scales better due to these interactions, and the parallelism of the
operation \cite{ALBA20027}. 

Having many populations offers researchers many configuration options and
additional challenges when designing efficient multi-population algorithms.
Options include the number and size of populations, the interaction between
them, the search area of each population, and the search strategy and
parametrization of each population.  In this paper, we are interested in the
latter, that is, having multiple populations running with distinct parameters
and optimization algorithms. We can find in the literature, several
heterogeneous multi-population algorithms integrating variations of optimization
algorithms, and these often perform better than single-population or homogeneous
multi-population optimization algorithms\cite{wu2016differential,nseef2016adaptive}.
Heterogeneous algorithms add to
the problem of finding the correct parameter settings for each population;
because some parameters affect the accuracy of the solution and the convergence
speed of the individual algorithms as they tip the balance between exploration
and exploitation of the search space. On the other hand, current studies show
that by having a high number of populations communicating in parallel, the
effect of the individual parameters of each population is compensated by those
selected in other populations \cite{li2016multi,tanabe2013evaluation}. Some
level of heterogeneity can be implemented by just changing the configuration
parameters of each population, but in this case, we are interested in
heterogeneous search strategies.

Therefore, in this paper, we implemented an asynchronous multi-population
algorithm version, using a message queue for inter-process communication \cite{
guervos2018introducing}, and a reactive migration procedure, in which we compare
three heterogeneous configurations using a randomized parameter technique. We
experimented with all populations using a GA or PSO search strategies, versus an
ensemble multi-population with both GA and PSO algorithms, using as a benchmark,
the first five functions of the BBOB testbed. We compare the options by
measuring the average running time (aRT) as the number of functions (\#FEs), as
the objective is to prove that the advantage of heterogeneous configurations
resides not only in the increased scalability but also in the search
performance. 

The organization of the paper is as follows: First, Section \ref{soa} presents
state of the art relevant to our work. In Section \ref{method}, we present the
proposed method.  Section \ref{setup} describes the design of the empirical
evaluation we designed to assess the effectiveness of the method, and in Section
\ref{results}, we report and discuss the results. Finally, we offer the
conclusions of this paper and suggestions on future work in Section
\ref{conclusions}.


\section{State of the Art}
\label{soa}

In general, population based algorithms have to keep a balance between
exploration and exploitation via the clever use of selection and
variation operators \cite{vcrepinvsek2013exploration}. In pursuit of
that objective, it is important to keep diversity high
\cite{yuan2005importance}, but different algorithms have different
mechanisms for increasing diversity (exploration) or decreasing it
(exploitation). PSO has two constants that rule in which direction the
{\em particle} is going to move: either randomly (exploration) or in
the direction of the best particle (exploitation); evolutionary
algorithms use mutation and, to a certain point, crossover for
exploration and crossover and selection procedures for
exploitation. Since these mechanisms are fundamentally different,
using several different algorithms at the same time might be
considered a win-win situation by way of performing exploitation in
several different directions at the same time, which will be able to
get closer to the solution as well as generate new possibilities.

This
is why hybrid population-based algorithms have been repeatedly
suggested in the literature: Pandi et al. \cite{pandi2011dynamic}
suggest a combination of swarm intelligence and another algorithm
denominated harmony search; Lien et al. \cite{lien2012hybrid} combine
particle swarm optimization with bee colony algorithms, while Zhao et
al. \cite{zhao2010hybrid} combine the latter with evolutionary
algorithms.

\begin{table}[h!tb]
    \caption{Equivalence of terms between particle swarm optimization
    and evolutionary algorithms. \label{tab:equivalence}}
\centering
\begin{tabular}{|c|c|}
  \hline
  PSO & EA \\ \hline
  Position vector & {\em genes} in a chromosome \\ \hline
  Move towards better & Selection  \\ \hline
  Random motion & Mutation \\ \hline
  Move towards centroid & Crossover \\ \hline
\end{tabular}
\end{table}
%
EAs and PSOs share another characteristic, besides the fact that they
act on populations: they can use the same data structures to
represent every member of the population. Please check Table
\ref{tab:equivalence} for equivalence of terms between PSO and EAs;
additionally to the data structures used, there is a certain
equivalence in the operators, but they are different enough to perform
search in different ways. The fact that they use the same data
structures means that you can apply them in turns, or simply start
calling a ``chromosome'' a ``particle'' or the other way round,
without any coversion, and keep working with it.

This is what the first papers to propose such a hybrid algorithm, by
Robinson et al. \cite{Robinson2002}, explicitly played on this
fact and also on what we might call diversity of local minima by
running an algorithm on a population until it became stagnated, and
then switching to the other one. This was done apparently only once,
but eventually the mixture of the two algorithms was able to obtain
better results in the design of a kind of antenna called ``horn'' than
each one of them separately, although they report that the best value
was obtained by the algorithm that started as a PSO and terminated as
an evolutionary one.

Another kind of hybrid was proposed independently by 
Shi et al. \cite{shi2003hybrid}, citing the ``local optimum'' problem,
that is, the same as the previous one; it mixes both algorithms
testing in different configurations: ``parallel'' and ``serial''
testing which configuration works better and is able to avoid
that. Their results on benchmark functions are mixed;
In general, however, a well-designed evolutionary or PSO
algorithm will not fall in that local optimum; it is certainly true
that, within a certain evaluation budget, neither might be able to
find that global optimum. However, it is probably that they mean that,
since exploitation of better-than-average solutions is done by the two
algorithms in different directions, a hybrid algorithm might help the
single-algorithm version to escape that.

This better exploitation capability was used by Grimaldi et al. \cite{grimaldi2005genetical}, with the objetive of solving electromagnetic problems. What they do is
to split the population into two different populations which will be
processed by an EA and a PSO algorithm; the new individuals generated
are merged in a single population, which is then split all over again
in the next iteration. This guarantees that none of the two algorithms
is getting stuck, since the individuals will be randomly subjected to
one or the other every generation.


Later on, Esmin et al. \cite{esmin2006hybrid} leveraged to design their
algorithms; this idea was
latter extended by Li et al. \cite{li2008gene}; in this work,
chromosomes/particles represent Support Vector Machines, which are
optimized to find the most representative features of gene expression
data sets. PSO and EAs are applied serially: 10 iterations of each,
after which the finalization criterium is examined; first PSO, then
EA. Separate EA and PSO are tested against this hybrid algorithm,
finding improvements of a few percent points in the accuracy over
classification of the whole dataset. This marginal, but significative,
improvement is essentially the kind of results that should be
expected. While improvements in diversity always boost the results by
allowing the algorithms to find better solutions, an
order-of-magnitude difference is not usually  achieved, since both
algorithm, by themselves, have good mechanisms for global
exploration.

Several other hybrid algorithms have been proposed more recently:
Gulia et al. \cite{gulia2019hybrid} mix swarm and EAs to select
software testing cases. % Maybe a bit of comment on how they actually
                        % do it.


\begin{figure*}
\includegraphics[width=5in]{kafkeo}
\caption{A sample black and white graphic
that needs to span two columns of text.}
\label{fig:kafkeo}
\end{figure*}

\section{Proposed Method}
\label{method}

% Write a small introduction here - JJ

\subsection{Architecture}
\label{arch}

In this section we present our proposed model for the execution of
multi-population based asynchronous algorithms. As we have mentioned before, when
designing efficient multi-population algorithms, we need to consider additional
issues \cite{Ma2019}, including the number and size of populations, how they
interact or communicate between them, and the search strategy and parametrization of
populations. We have considered these requirements and, in this paper,
propose a cloud-native
multi-population solution. In this model, populations are the primary data
structure, and we package them as messages that are part of a continuous stream flowing
from one computing node, performing an algorithm, to the next. To achieve this ``continuous stream,''
everything must happen asynchronously without computing node needing to wait for
others, and becoming available as soon as it is needed.

We have implemented this streaming functionality by using a message queue
system, whose general architecture is shown in Figure \ref{fig:kafkeo}. We can explain
the model by using an analogy of producers and consumers of messages. There are
two queues, one labeled \texttt{input}, and the other one \texttt{output}. In a
queue, {\em push} operations are represented by solid arrows connecting to the
left side and pull or pop operations as solid arrows leaving from the right
side. The \texttt{controller} process, is responsible for the migration of
individuals from one population to the other, and keeping track of the
iterations of the algorithm. There is at least one \texttt{stateless function}
worker process responsible for running the isolated algorithms. The two
processes \texttt{PSO} and \texttt{GA}, are shown.

We can follow the path of messages as follows:

\begin{enumerate}

\item In this step, the specified number of populations are created according
to the configuration. Populations at this moment are just static data messages,
including each individual inside. Each population includes a metadata section
where its algorithm and execution parameters are specified. For instance, for a
GA, the mutation rate, type of crossover operator, and other values are
indicated.

\item The setup process pushes each population on to the \texttt{input}
queue so that they can be consumed by stateless functions responsible for the
execution of the search.

% before you have used Function in caps - JJ
\item One or more \texttt{stateless functions} are constantly pulling
population messages from the \texttt{input} queue. These functions have the
task of executing the optimization algorithm. They take the current state of
the population and start to run the specified algorithm for a certain number of
iterations.

\item Once they finish the execution, the resulting populations are pushed to
the \texttt{output} queue; when the computing node is done with a population, it draws another one from the
queue.

% You are using also caps here for the queue name - JJ
\item The \texttt{controller} pulls populations from the \texttt{output} queue,
inspects the metadata describing the current state of the populations, and if
the algorithm has found an optimal solution or the maximum number of iteration
has been reached it stops the execution.

\item Otherwise, it passes the stream of messages to a migration function,
where populations are mixed. Migration generates new populations, and they are
again pushed to the \texttt{input} queue to continue in a loop. The
\texttt{controller} is also responsible for logging the metadata.

\end{enumerate}

This reactive architecture has the following advantages:
\begin{itemize}

\item It decouples the population and the population-based algorithm. In this
design, we can have one or many processing nodes, each running a different
search strategy.

\item Also, the reactive controller gives designers more control over the
multi-population algorithm. In this process, designers can dynamically change
the number of populations, population parameters, and migration details
on-the-fly.

\item Another advantage is that algorithm designers have many options for
implementing this simple architecture. The same basic components can be
implemented as a single multi-threaded program, or as a highly scalable
serverless cloud application.
\end{itemize}

\subsection{Stateless functions} 
\label{functions} 

% Maybe mention that before? You have already mentioned them above - JJ
This message queue pattern is an essential component of a highly scalable
reactive architecture; one of the reasons for this scalability is the use of
stateless functions that have no secondary effects, or the need to read data
from an external entity. Stateless functions do not need to read, keep, or
modify data outside of the scope of the method, it will have no side
effects reading inputs and producing an output, mapping input to ouput
as it were. If we implement population-based
optimizarion algorithms using stateless functions, then to the system, there is
no difference between having one or many copies of the same function pulling
work from the queue all at the same time, because there are no unwanted side
effects, including problems of resource locking resulting from this
concurrency. % Add a few references to our papers, anonymized or not - JJ

To have a stateless version of a population-based algorithm, we only need to
skip the step of creating a random population. Instead, the function receives
the population as a parameter along with the required parameters. After several
iterations (specified in the parameters), the function returns the current state
of the population, with additional data describing the local execution. This
data is necessary to log the \#FE and current fitness values.


\subsection{Migration}
\label{migration}
%% TO DO:
%% Explain migration

As the controller is pulling population messages from the output queue, it waits
until it takes three valid populations to trigger an event handled by the
\texttt{population\_mixer}  function, that takes a list containing the three
populations. This reactive design has the advantage of not needing to keep a
buffer in memory or external storage, and it follows the reactive paradigm. A
possible disadvantage can be that it only mixes contiguous populations, but we
could mitigate this with a larger buffer.

The \texttt{population\_mixer} receives a list of three populations, let us say
[A, B, C], and calls the migration method shown in Algorithm \ref{alg:migration}
for [A, B], [B, C] and [A, C]. The migration algorithm sorts both populations
and generates a new population containing the best half from each. Finally, the
method pushes the new populations to the \texttt{messages Observable}. From
there, a \texttt{publish(population)} observer is responsible for pushing the
newly generated populations back to the \texttt{input} message queue.

\begin{algorithm}
    \caption{Migration}
    \label{alg:migration}
    \begin{algorithmic}[1]
        \Procedure{cxBestFromEach}{$pop_1,pop_2$}
            \State $pop_1.sort()$
            \State $pop_2.sort()$
            \State $size\gets min(len(pop_1), len(pop_2))$
            \State $cxpoint\gets (size-1)/2$
            \State $pop_1[cxpoint:]\gets pop_2[:cxpoint+2]$
            \State \textbf{return} $pop_1$
        \EndProcedure 
    \end{algorithmic}
\end{algorithm}

\section{Experimental setup}
\label{setup}

% We don't verify, unless you will be comenting the results: you will
% be explaining how the system to verify is going to be set up - JJ
% Yes, you are right - Mario
In this section, present the experimental setup needed 
to verify if having a multi-population algorithm, with
heterogeneous populations and the support for mixing search strategies,
increases the performance of the search by needing fewer function evaluations
than a homogeneous setting.

For the experiment, we used five benchmark separable functions ($f_1 to f_5 $)
from the Continuous Noiseless BBOB testbed, which is part of the Comparing
Continuous Optimizers (COCO) framework \cite{hansen2016coco}. The functions are
real-parameter, single-objective benchmark functions. We tested with fifteen
instances of each function, each having a different optimal value. The standard
benchmark of the testbed uses these 15 instances over 2, 3, 5, 10, 20, and 40
dimensions. With the maximum number of function evaluations (\#FEs) increasing
with the dimension (D), using the expression $10^5 \cdot D$ (i.e. for $D = 2$,
\#FEs is $200,000$).

The COCO framework offers several tools to compare the performance of
algorithms, generating data sets, tables, and reports for an experiment, we
compared using the average required time (aRT). There is a
repository\footnote{\url{https://coco.gforge.inria.fr/doku.php?id=algorithms-bbob}}
of more than 200 results for the noiseless BBOB testbed, collected from BBOB
workshops and special sessions between the years 2009 and 2019. To test the
heterogeneous multi-population capabilities, we compare the aRT between a
homogeneous and an ensemble of multi-populations, using Genetic Algorithms
(GAs) and Particle Swarm Optimization (PSO). We used the DEAP framework for the
GA stateless function \cite{fortin2012deap} and the EvoloPy library
\cite{faris2016evolopy} for the PSO algorithm function. We show the parameters
for each algorithm, in Tables \ref{tab:GAparams} and Table \ref{tab:PSOparams}
for the GA and PSO, respectively. We obtained these parameters by following a
method we used in a previous work \cite{garcia2017benchmarking}. First, we
tested the parameters for the Rastrigin separable function with five
dimensions. After about fifteen experiments, the most challenging targets were
achieved for this particular function. We tested again with functions one to
three, and after obtaining favorable results, we set the PSO and GA algorithm
parameters. We set the mutation and crossover probabilities of the GA randomly
to have heterogeneous populations; the table shows the range of these
parameters. We did not change these settings during the experiments, and we
only provided the population size and number of generations as parameters.

\begin{table}
  \small
  \caption{ DEAP GA EvoWorker Parameters }
  \label{tab:GAparams} 
  \centering
  \small
  \begin{tabular}{|l|c|}
    \hline
    Selection & Tournament size=12                            \\ \hline
    Mutation & Gaussian $\mu=0.0$, $\sigma=0.5$, indbp=0.05   \\ \hline
    Mutation Probability & [0.1,0.3]                          \\ \hline
    Crossover & Two Point                                     \\ \hline
    Crossover Probability  & [0.2,0.6]                          \\ \hline
  \end{tabular}
\end{table}

\begin{table}
  \small
  \caption{ EvoloPy PSO Parameters }
  \label{tab:PSOparams} 
  \centering
  \small
  \begin{tabular}{|l|c|}
    \hline
    $V_{max}$ & 6 \\ \hline
    $W_{max}$ & $0.9$ \\ \hline
    $W_{min}$ & $0.2$ \\ \hline
    $C_1$ & 2 \\ \hline
    $C_2$ & 2 \\ \hline
  \end{tabular}
\end{table}

To run the experiments, we deployed the docker application with 8 worker
containers hosting the stateless version of the GA and PSO algorithms described
earlier and a total of 10 populations. Table \ref{tab:params} shows the
parameters we used. The {\em GA-PSO Ratio} parameter specifies the proportion of
populations that will use the GA algorithm. Next, we specify that the experiment
will use only the first five functions, and how many instances. {\em Instances}
has a default value of 15. We used this value because it is the standard for the
BBOB benchmark \cite{hansen2016coco}. We define a list of the dimensions that we
will test in the {\em Dimensions} parameter. Then, we defined for each
dimension: the number of populations, and for each population,  the number of
generations and population size. Finally, we define how many complete loops the
algorithm will perform. The product of these parameters gives us a maximum
\#FEs. For example, for $D = 2$, the maximum number evaluations is $200,000$,
this is the same as $40*50*10*10$.

\begin{table}
  \small
  \caption{Parameters used in the experiments with ten populations
  }
  \label{tab:params:10}
  \vspace{0.25cm}
  \centering
  \small
  \begin{tabular}{|l|c|c|c|c|c|c|}
    \hline
    Dimension        & 2  & 3  & 5  & 10 & 20  & 40  \\ \hline
    Generations      & 40 & 25 & 28 & 50 & 66  & 80  \\ \hline
    Population Size  & 50 & 60 & 60 & 70 & 100 & 125 \\ \hline
    Populations      & 10 & 10 & 10 & 10 & 10  & 10  \\ \hline
    Iterations       & 10 & 20 & 30 & 30 & 30  & 40  \\ \hline  
  \end{tabular}
\end{table}

\begin{figure}[h!tb]
  \begin{tabular}
      {c@{\hspace*{-0.00001\textwidth}}
      % c@{\hspace*{-0.00001\textwidth}}
      }
     
  \includegraphics[width=0.28\textwidth]{ppfigs_f001}\\
  \includegraphics[width=0.28\textwidth]{ppfigs_f002}\\

  \includegraphics[width=0.28\textwidth]{ppfigs_f003}\\
  \includegraphics[width=0.28\textwidth]{ppfigs_f004}\\
  
  \includegraphics[width=0.28\textwidth]{ppfigs_f005}\\
  \end{tabular}
  \vspace{-3ex}
   \caption{Average running time (in \#FEs as $log_{10}$ value),
    divided by dimension for target function value $10^{-8}$ vs dimension. 
    Algorithms legends are given in $f_1$. Light symbols give the maximum number of 
    function evaluations from the longest trial divided by dimension. 
    Black stars indicate a statistically better result compared to 
    all other algorithms ($p < 0.01$) and Bonferroni 
    correction number of dimensions (six).}
\end{figure}  

We deployed the container-based application in a
Dell server,  with dual Intel CPUs E5-2640V3 @ 2.60 GHz, 32 threads,
and 32 GB RAM. We used Docker version 2, in Ubuntu Linux 18.04, and Python
(version 3.7) code. Container images and Docker compose file are available at
(https://hub.docker.com/x/xx/), and (https://github.com/x/xx/). 
The experiments were performed with COCO , version bbob.v15.03 in python, 
the plots were produced with version 2.3.2.


\section{Results}
\label{results}

The results obtained show (see Figure ) that the proposed algorithm reaches the
$10^{-8}$ target value on all functions ($f_1-f_5$), and scales well to higher
dimensions. On the other hand, the GA algorithm has a hard time reaching even
the $10^{-5}$ target value in functions ($f_1-f_4$) but performs well on
function $f_5$. The multi-population PSO has a better performance, but it is not
able to reach the  $10^{-8}$ target on the higher dimensions of $f_3$ and $f_4$.

These results are also competitive when compared against other GA and PSO
implementations of past BBOB workshops. In particular, the simple binary GA algorithm
of Nicolau \cite{nicolau2009application}, the PSO algorithm by El-Abd and Kamel \cite{el2009black} 
and an EDA and PSO hybrid \cite{el2009blackHybrid}.



% \subsection{Experiment}
% \label{sec:exp_results}
% Having a multi-population based algorithm can decrease the execution time
% because of the parallel execution of the evolution. But, having heterogeneous 
% populations might enhance evolutionary search and needless
% evaluations than homogeneous systems; heterogeneous settings, if
% done right, increases the diversity of the whole population
% \cite{araujo2008multikulti}.

% But this is a rule of thumb, and it will depend on the degree of heterogeneity,
% as well as on the algorithm itself. Some level of heterogeneity can be
% implemented by just changing the configuration parameters of each population,
% but in this case, we are interested in a heterogeneous search strategy. 
% Therefore, in this experiment we compare a multi-population with only GA and PSO populations,
% versus an ensemble of GA and PSO algorithms. We tested on the first five functions of the
% BBOB testbed. We used ten populations and eight workers for the experiment and the
% same parameters as before.  


\begin{figure*}[h!tb]
    \begin{tabular}
        {c@{\hspace*{-0.00001\textwidth}}
         c@{\hspace*{-0.00001\textwidth}}
         c@{\hspace*{-0.00001\textwidth}}
        }
    GA  &  PSO & GA \& PSO\\   
    \includegraphics[width=0.28\textwidth]{GAOnly_f001}&
    \includegraphics[width=0.28\textwidth]{PSOOnly_f001}&
    \includegraphics[width=0.28\textwidth]{GAPSO_f001}\\

    \includegraphics[width=0.28\textwidth]{GAOnly_f002}&
    \includegraphics[width=0.28\textwidth]{PSOOnly_f002}&
    \includegraphics[width=0.28\textwidth]{GAPSO_f002}\\

    \includegraphics[width=0.28\textwidth]{GAOnly_f003}&
    \includegraphics[width=0.28\textwidth]{PSOOnly_f003}&
    \includegraphics[width=0.28\textwidth]{GAPSO_f003}\\

    \includegraphics[width=0.28\textwidth]{GAOnly_f004}&
    \includegraphics[width=0.28\textwidth]{PSOOnly_f004}&
    \includegraphics[width=0.28\textwidth]{GAPSO_f004}\\

    \includegraphics[width=0.28\textwidth]{GAOnly_f005}&
    \includegraphics[width=0.28\textwidth]{PSOOnly_f005}&
    \includegraphics[width=0.28\textwidth]{GAPSO_f005}\\
    \end{tabular}
    \vspace{-3ex}
     \caption{
 Scaling of runtime with dimension to reach certain target values $\Delta f$. Lines:
 average runtime (aRT); Cross ($+$): median runtime of successful runs to reach
 the most difficult target that was reached at least once (but not always);
 Cross (\textcolor{red}{$\times$}): maximum number of f-evaluations in any trial. Notched boxes:
 interquartile range with median of simulated runs; All values are divided by
 dimension and plotted as $log_{10}$ values versus dimension. 
 %Shown is the aRT for
 %fixed values of $\Delta f = 10k$ with $k$ given in the legend. 
 Numbers above aRT-symbols
 (if appearing) indicate the number of trials reaching the respective target.
 %The light thick line with diamonds indicates the best algorithm from BBOB 2009
 %for the most difficult target. Horizontal lines mean linear scaling, slanted
 %grid lines depict quadratic scaling.
 }
\end{figure*}




\section{Conclusions}
\label{conclusions}

\begin{acks}

  The authors would also like to thank the anonymous referees for
  their valuable comments and helpful suggestions. The work is
  supported by the 
\end{acks}
